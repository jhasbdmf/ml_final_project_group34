{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f6f88b",
   "metadata": {},
   "source": [
    "**MLP USING NUMPY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d48ab04471e4d8",
   "metadata": {},
   "source": [
    "side note: we ran the code in a seperate py file, as it was easier to handle the big data amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1428b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.290640Z",
     "start_time": "2025-08-22T09:19:50.872115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import math\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6951fe-5c32-4425-b2df-0348f976679b",
   "metadata": {},
   "source": [
    "A method to print strings into a file with filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fcd2f2-c611-4e38-8353-54ec0cf6b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(message):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4607f-4283-434e-a622-584b0a5ee6bc",
   "metadata": {},
   "source": [
    "#dataset preprocessing methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7d8b61-b7fe-45cd-836b-3d3e72e885bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.290640Z",
     "start_time": "2025-08-22T09:19:50.872115Z"
    }
   },
   "outputs": [],
   "source": [
    "#Normalizing the pixel values so that no pixel gets too important\n",
    "#for the model when it should not.\n",
    "def z_normalize_images(images):\n",
    "    mean = images.mean()\n",
    "    std  = images.std()\n",
    "    eps  = 1e-8\n",
    "    return (images - mean) / (std + eps)\n",
    "\n",
    "    \n",
    "#read training set, 80% of the original dataset\n",
    "#reads the rest 20% of the dataset and splits\n",
    "#those 20% into validation and test set\n",
    "def preprocess_dataset_for (ann_type: str):\n",
    "    train_x = np.load(f'train_{ann_type}_x.npy') \n",
    "    train_y = np.load(f'train_{ann_type}_y.npy')\n",
    "    val_and_test_x  = np.load(f'test_{ann_type}_x.npy')   \n",
    "    val_and_test_y  = np.load(f'test_{ann_type}_y.npy')\n",
    "\n",
    "    N = len(val_and_test_x)\n",
    "    perm = np.random.RandomState(seed=42).permutation(N)\n",
    "    split_at = N // 2\n",
    "\n",
    "    val_idx = perm[:split_at]\n",
    "    test_idx = perm[split_at:]\n",
    "\n",
    "    val_x  = val_and_test_x[val_idx]\n",
    "    val_y  = val_and_test_y[val_idx]\n",
    "    test_x = val_and_test_x[test_idx]\n",
    "    test_y = val_and_test_y[test_idx]\n",
    "\n",
    "        # z_normalization of inputs\n",
    "    train_x  = z_normalize_images(train_x)  \n",
    "    val_x  = z_normalize_images(val_x)  \n",
    "    test_x  = z_normalize_images(test_x)  \n",
    "\n",
    "    #create training, val and test set by zipping \n",
    "    #corresponding inputs and targets\n",
    "    training_set = zip(train_x, train_y)\n",
    "    val_set = zip(val_x, val_y)\n",
    "    test_set = zip(test_x, test_y)\n",
    "\n",
    "    return training_set, val_set, test_set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67edd152",
   "metadata": {},
   "source": [
    "The model built is a multi-layer perceptron (MLP). The networks architecture is defined in __init__. The input dimension is 2304 (48x48 pixels flattened), stemming from the input images. Upon initialization, an MLP model has n_layers hidden ReLU layers with hidden_dim neurons per layer. Output layer of the network can be interpreted as indicating which of the target classes the network input falls under. The higher the output of a neuron in the final network layer, the greater the likelihood that inputs fall under the target class which corresponds to that neuron. Here, we deal with 7 classes, one for each facial expression. \n",
    "\n",
    "Weights oh hidden ReLU layers are initilized via the Kaiming method to get better value of the CEL during training and run into numerical issues less frequently. I am not sure what the latter effect has to do with the initialization scheme, empirically Kaiming initialization helped me avoid numerical errors until the hidden dimension of at least 335. Last layer weights are sampled from the normal distribution with mean of 0 and standard deviation of 0.2.\n",
    "\n",
    "When a target value of a certain vector of the input space is fed into the network on top of that input space vector, softmax normalizes logits viz. outputs of neurons in the final network layer to turn them into probabilities. If no target value is fed into the network, it returns logits i.e. unnormalized values for each of the target classes. Model prediction as to the target class of the input image is then the target class with the highest logit. Using the numerically greatest logis to determine target class saves computational recources during inference.\n",
    "\n",
    "Finally, the model backpropagates the losses by calculating the gradient of cross-entropy loss (CEL) wrt each of the hidden layer parameter matrices. Some of the computations to determine the gradient of CEL wrt to more posterior network layers are reused to compute the gradient of CEL wrt more prior layers thereof. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b440b94b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.348589Z",
     "start_time": "2025-08-22T09:19:52.329512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP ():\n",
    "    #input layer is not counted in n_layers\n",
    "    #output layer is\n",
    "    def __init__(self, input_dim=2304, n_layers=5, hidden_dim=32, n_classes=7):\n",
    "        rng = np.random.default_rng(seed=42) \n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = []\n",
    "        for i in range (n_layers):\n",
    "            in_dim = hidden_dim\n",
    "            out_dim = hidden_dim\n",
    "            if i == 0:\n",
    "                in_dim = input_dim\n",
    "            elif (i+1) == n_layers:\n",
    "                out_dim = n_classes\n",
    "            std = 0.2\n",
    "            if in_dim == out_dim:\n",
    "                std = np.sqrt(2/in_dim)\n",
    "            current_layer = rng.normal(\n",
    "                #mean\n",
    "                loc=0.0,      \n",
    "                #standard deviation\n",
    "                scale=std,        \n",
    "                size=(in_dim, out_dim)\n",
    "            ).astype(np.float32)\n",
    "            self.layers.append (current_layer)\n",
    "\n",
    "    def ReLU (self, x):\n",
    "        x = np.asarray(x)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def _get_normalized_logits_with_softmax_denom(self, logits):\n",
    "        logits = logits - np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        softmax_denominator = np.sum(exp_logits)\n",
    "        return exp_logits / softmax_denominator, softmax_denominator\n",
    "    \"\"\"\n",
    "    def _get_normalized_logits_with_softmax_denom(self, logits):\n",
    "        softmax_denominator = np.sum(np.exp(logits))\n",
    "        return np.exp(logits) / softmax_denominator, softmax_denominator\n",
    "    \"\"\"\n",
    "    def forward(self, inputs, target_value=None, requires_grad=False):\n",
    "        hidden_layer_activations = []\n",
    "        #layer_activations.append(inputs.copy())\n",
    "     \n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            \n",
    "             # Special case: only one layer, it’s the output layer\n",
    "            if self.n_layers == 1:\n",
    "                logits = inputs @ self.layers[i]\n",
    "            #ReLU activations in all the layers but the last\n",
    "            elif i == 0:\n",
    "                hidden_layer_activations.append(self.ReLU(inputs @ self.layers[i]))\n",
    "                \n",
    "            elif (i+1) < self.n_layers:\n",
    "                hidden_layer_activations.append(self.ReLU(hidden_layer_activations[i-1] @ self.layers[i]))\n",
    "            #no activation function applied so far\n",
    "            #in the last layer\n",
    "            #softmax will be applied to the output \n",
    "            #of the last layer later if necessary\n",
    "            else:\n",
    "                logits = hidden_layer_activations[i-1] @ self.layers[i]\n",
    "\n",
    "        #if no target value is passed to the model.forward\n",
    "        #then the model is in the inference mode\n",
    "        #no logit/output normalization/softmax is necessary\n",
    "        #in the inference mode\n",
    "        #since one can base model prediction on the highest\n",
    "        #unnormalized/unsoftmaxed logit\n",
    "        if target_value is None:\n",
    "            return logits\n",
    "        else:\n",
    "            #this is softmax\n",
    "            #softmax denominator is returned by softmax \n",
    "            #on top of normalized logits to be\n",
    "            #reused in computing CEL\n",
    "            normalized_logits, softmax_denom = self._get_normalized_logits_with_softmax_denom(logits)\n",
    "\n",
    "            \n",
    "            #CEL is equal to -ln(exp(logits[target_value])/softmax_denom))\n",
    "            #the formula below is algebraically equivalent to the one above\n",
    "            #CEL_value = -logits[target_value] + np.log(softmax_denom)\n",
    "            CEL_value = -np.log(normalized_logits[target_value])\n",
    "            \n",
    "        #if no gradient is required,\n",
    "        #then just return CEL\n",
    "        if not requires_grad:\n",
    "            return CEL_value\n",
    "        \n",
    "\n",
    "        #if a target value is passed to model.forward\n",
    "        #and CEL is required\n",
    "        #then the model is in the training mode\n",
    "        #one needs to do softmax on the inputs\n",
    "        #to pass softmaxed logits into \n",
    "        #CEL/cross-entropy loss\n",
    "        else:\n",
    "\n",
    "            #this one is the gradient of CEL\n",
    "            #d_softmax = normalized_logits.copy()\n",
    "            d_softmax = normalized_logits\n",
    "            d_softmax[target_value] -= 1\n",
    "\n",
    "            #initialize an list with as many empty\n",
    "            #elements as there are hidden layers\n",
    "            #i.e. param matrices\n",
    "            layer_gradients = [None] * self.n_layers\n",
    "\n",
    "            #this computes gradients of layer params\n",
    "            for i in range(self.n_layers-1, -1, -1):\n",
    "\n",
    "                if self.n_layers == 1:\n",
    "                    # Single-layer network\n",
    "                    dynamic_gradient = d_softmax  # from your loss derivative\n",
    "                    layer_gradients[i] = np.outer(inputs, dynamic_gradient)\n",
    "                #output softmax layer\n",
    "                elif i == (self.n_layers-1):\n",
    "                    #this is the part of the gradient\n",
    "                    #which is reused across layers\n",
    "                    dynamic_gradient = d_softmax\n",
    "\n",
    "                    #this one contrains gradient of i-th layer\n",
    "                    layer_gradients[i] = np.outer(hidden_layer_activations[i-1], dynamic_gradient)\n",
    "                else:\n",
    "                    #this one multiplies dynamic gradient by the gradient of pre-activations\n",
    "                    #gradient of pre-activations is equal to the corresponding weight matrix\n",
    "                    #which us stored in self.layers[i+1]\n",
    "                    dynamic_gradient = self.layers[i+1] @ dynamic_gradient\n",
    "                    #gradient of hidden ReLU activation is equal to 1\n",
    "                    #if that activation is equal to 1 and 0 otherwise \n",
    "                    relu_grad = (hidden_layer_activations[i]>0).astype(float)\n",
    "                    #mulitply dynamic grad by activation gradient\n",
    "                    dynamic_gradient *= relu_grad\n",
    "                    #input layer\n",
    "                    if i == 0:\n",
    "                        layer_gradients[i] = np.outer(inputs, dynamic_gradient)\n",
    "                    #neither output nor input layer\n",
    "                    else:\n",
    "                        layer_gradients[i] = np.outer(hidden_layer_activations[i-1], dynamic_gradient)\n",
    "            return CEL_value, layer_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812c9cf",
   "metadata": {},
   "source": [
    "The next function runs teh model on a dataset and computes the overall loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb46a501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.391430Z",
     "start_time": "2025-08-22T09:19:52.383042Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on(model, dataset):\n",
    "    total_loss = 0\n",
    "    for x, y in dataset:\n",
    "        total_loss += model.forward(x, y, requires_grad=False)\n",
    "    return total_loss/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1e0b6",
   "metadata": {},
   "source": [
    "The model is trained via stochastics gradient descent. The batch size is thus equal to 1. After each epoch, the learning rate is multiplied by an antecedently specified hyperparameter whose value is constant during training and is smaller than 1. This measure is introduced so that the optimizer is not vulnerable to a scenario in which it constantly overshoots a local minimum of CEL and thus diverges therefrom. A decreasing learning rate/step size ensures that a local minimum of CEL is being overshot less and less as model training is progressing.\n",
    "\n",
    "The optimizer retuns the state of the model which achieved the lowest CEL score on the validation set as opposed to the state of the model which achieved the lowest CEL score on the validation set after n_epochs epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15e33b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.416023Z",
     "start_time": "2025-08-22T09:19:52.407323Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_SGD (model, \n",
    "                         training_set,\n",
    "                         validation_set,\n",
    "                         lr: float, \n",
    "                         n_epochs: int, \n",
    "                         sgd_lr_multiplier: float = 0.95\n",
    "                        ):\n",
    "    \n",
    "\n",
    "    print (\"_\" * 50)\n",
    "    print (f\"Initial LR = {lr}\")\n",
    "    print (f\"LR multipliter per epoch = {sgd_lr_multiplier:.5f}\")\n",
    "    print (f\"Number of layers = {model.n_layers}\")\n",
    "    print (f\"Dimensionality of hidden layers = {model.hidden_dim}\")\n",
    "    print (\"_\" * 50)\n",
    "\n",
    "    log_message (\"_\" * 50)\n",
    "    log_message (f\"Initial LR = {lr}\")\n",
    "    log_message (f\"LR multipliter per epoch = {sgd_lr_multiplier:.5f}\")\n",
    "    log_message (f\"Number of layers = {model.n_layers}\")\n",
    "    log_message (f\"Dimensionality of hidden layers = {model.hidden_dim}\")\n",
    "    log_message (\"_\" * 50)\n",
    "\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_avg_epoch_loss = 1000\n",
    "    #best_model = copy.deepcopy(model)\n",
    "\n",
    "    for epoch_index in range(1, n_epochs + 1):\n",
    "\n",
    "        print (f\"Epoch {epoch_index}/{n_epochs}\")\n",
    "        print (f\"current SGD learning rate = {lr}\")\n",
    "\n",
    "        log_message (f\"Epoch {epoch_index}/{n_epochs}\")\n",
    "\n",
    "        log_message (f\"current SGD learning rate = {lr}\")\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        #shuffle training set in a reproducible manner\n",
    "        random.seed(42 + epoch_index)\n",
    "        random.shuffle(training_set) \n",
    "\n",
    "        for x,y in training_set:\n",
    "\n",
    "            #get the CEL gradient from the forward pass directly\n",
    "            loss, layer_grads = model.forward(x, y, requires_grad=True)\n",
    "         \n",
    "            #do SGD step\n",
    "            for i in range(model.n_layers):\n",
    "                model.layers[i] -= lr*layer_grads[i]\n",
    "\n",
    "            total_train_loss += loss\n",
    "          \n",
    "        #decrease lr each epoch\n",
    "        lr *= sgd_lr_multiplier\n",
    "        \n",
    "        #compute, print and save avg loss per epoch\n",
    "        avg_train_loss = total_train_loss / len(training_set)\n",
    "        print (f\"average train loss = {avg_train_loss:.5f}\")\n",
    "        log_message (f\"average train loss = {avg_train_loss:.5f}\")\n",
    "        train_loss_history.append(avg_train_loss)\n",
    " \n",
    "        avg_val_loss = evaluate_model_on (model, validation_set)\n",
    "        print (f\"average val loss = {avg_val_loss:.5f}\")\n",
    "        log_message (f\"average val loss = {avg_val_loss:.5f}\")\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_avg_epoch_loss:\n",
    "            best_avg_epoch_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print (\"_\" * 50)\n",
    "        log_message (f\"average val loss = {avg_val_loss:.5f}\")\n",
    "    if best_model is not None:\n",
    "        return best_model, train_loss_history, val_loss_history\n",
    "    else:\n",
    "        return model, train_loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb06271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:19:52.441723Z",
     "start_time": "2025-08-22T09:19:52.435653Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_history, val_loss_history):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d31476",
   "metadata": {},
   "source": [
    "Grid search tries out several combinations of hyperparameters in order to achieve the lowest validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a462ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:20:02.946908Z",
     "start_time": "2025-08-22T09:20:02.938905Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(hyperparameters: dict,\n",
    "                train_set: list,\n",
    "                validation_set: list,\n",
    "                n_epochs: int):\n",
    "    best_loss = float(\"inf\")\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "\n",
    "    # extract hyperparameter combinations\n",
    "    learning_rate = hyperparameters.get('lr', [])\n",
    "    lr_multipliers = hyperparameters.get('lr_multiplier', [])\n",
    "    hidden_dims = hyperparameters.get('hidden_dim', [])\n",
    "    n_layers_list = hyperparameters.get('n_layers', [])\n",
    "\n",
    "    # iterate over all combinations\n",
    "    for lr in learning_rate:\n",
    "        for lr_multiplier in lr_multipliers:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                for n_layers in n_layers_list:\n",
    "                    print (\"_\" * 100)\n",
    "                    print(f\"Testing: lr={lr}, lr_multiplier={lr_multiplier}, \"\n",
    "                          f\"hidden_dim={hidden_dim}, n_layers={n_layers}\")\n",
    "                    \n",
    "                    log_message (\"_\" * 100)\n",
    "                    log_message (f\"Testing: lr={lr}, lr_multiplier={lr_multiplier}, \"\n",
    "                          f\"hidden_dim={hidden_dim}, n_layers={n_layers}\")\n",
    "\n",
    "                    # create a new model for each combination\n",
    "                    model = MLP(n_layers=n_layers, hidden_dim=hidden_dim)\n",
    "\n",
    "                    # train the model and track validation loss history\n",
    "                    trained_model, train_loss_history, val_loss_history = train_model_with_SGD(\n",
    "                        model,\n",
    "                        train_set,\n",
    "                        validation_set,\n",
    "                        lr=lr,\n",
    "                        n_epochs=n_epochs,\n",
    "                        sgd_lr_multiplier=lr_multiplier\n",
    "                    )\n",
    "\n",
    "                    # find the best validation loss\n",
    "                    min_val_loss = min(val_loss_history)\n",
    "                    #print (f\"min_val_loss = {min_val_loss:.5f}\")\n",
    "\n",
    "                    # check if new combination is best\n",
    "                    if min_val_loss < best_loss:\n",
    "                        best_loss = min_val_loss\n",
    "                        best_params = {\n",
    "                            'lr': lr,\n",
    "                            'lr_multiplier': lr_multiplier,\n",
    "                            'hidden_dim': hidden_dim,\n",
    "                            'n_layers': n_layers\n",
    "                        }\n",
    "                        best_model = trained_model\n",
    "                        best_train_loss_history = train_loss_history\n",
    "                        best_val_loss_history = val_loss_history\n",
    "                    \n",
    "                    print (f\"min best val loss so far = {best_loss}\")\n",
    "                    log_message (f\"min best val loss so far = {best_loss}\")\n",
    "              \n",
    "\n",
    "    return best_train_loss_history, best_val_loss_history, best_model, best_params, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a7823",
   "metadata": {},
   "source": [
    "Splitting datasets into validation and training data, input and target values. The data is shuffled with a fixed seed in order to take out any unwanted data structure that may arise from grouped or sorted data. Inputs are normalized and decimal notation is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa831ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T09:20:32.289654Z",
     "start_time": "2025-08-22T09:20:15.892539Z"
    }
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"mlp_grid_search_log_{timestamp}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"=== New Log Start ===\\n\")\n",
    "\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = preprocess_dataset_for(\"mlp\")\n",
    "\n",
    "train_set = list(train_set)\n",
    "val_set = list (val_set)\n",
    "test_set = list (test_set)\n",
    "\n",
    "#numpy printing instruction for decimal notation\n",
    "np.set_printoptions(\n",
    "    precision   = 5,       \n",
    "    floatmode   = 'fixed',  \n",
    "    suppress    = True     \n",
    ")\n",
    " \n",
    "\n",
    "#create training, val and test set by zipping \n",
    "#corresponding inputs and targets\n",
    "#training_set = list(train_set)\n",
    "#val_set = list(val_set)\n",
    "#test_set = list(test_set)\n",
    "\n",
    "#numpy printing instruction for decimal notation\n",
    "np.set_printoptions(\n",
    "    precision   = 5,       \n",
    "    floatmode   = 'fixed',  \n",
    "    suppress    = True     \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b0b98",
   "metadata": {},
   "source": [
    "Originally, grd search was performed on 27 different combinations of hyperparameters. The learning rates of 0.001, 0.005 and 0.01 as well as hidden dimensions i.e. width of hidden layers of 16, 32 and 64 were tested. On the 22nd of August 2025, the best validation loss was achieved for 7 hidden layers with 32 neurons each via the learning rate of 0.005. You can see complete grid search log in the file mlp_grid_search_log_20250822_111958.txt. A different result can be obtained on a different day even with the random seed.\n",
    "\n",
    "There is no need to test 27 combinations of hyperparameters again to demonstrate that grid search works. We only test networks of varying depth with different hidden dimension of 32 or 64 with the learning rate of 0.005. \n",
    "\n",
    "Such reduced grid search is sufficient to demonstrate that increasing the depth of MLP is helpful at least as long as no vanishing gradient problem is encountered. Apparently, increasing network width is also a path of extremely diminishing returns. There may be an upper bound of efficiency one gains via increasing network width. This network width upper bound may have to do with the complexity of the problem an MLP is trained to solve.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e4d7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Testing: lr=0.005, lr_multiplier=0.95, hidden_dim=32, n_layers=1\n",
      "__________________________________________________\n",
      "Initial LR = 0.005\n",
      "LR multipliter per epoch = 0.95000\n",
      "Number of layers = 1\n",
      "Dimensionality of hidden layers = 32\n",
      "__________________________________________________\n",
      "Epoch 1/10\n",
      "current SGD learning rate = 0.005\n",
      "average train loss = 9.05477\n",
      "average val loss = 9.53613\n",
      "__________________________________________________\n",
      "Epoch 2/10\n",
      "current SGD learning rate = 0.00475\n",
      "average train loss = 7.81578\n",
      "average val loss = 7.46549\n",
      "__________________________________________________\n",
      "Epoch 3/10\n",
      "current SGD learning rate = 0.0045125\n",
      "average train loss = 7.16407\n",
      "average val loss = 8.11160\n",
      "__________________________________________________\n",
      "Epoch 4/10\n",
      "current SGD learning rate = 0.004286875\n",
      "average train loss = 6.71783\n",
      "average val loss = 6.60034\n",
      "__________________________________________________\n",
      "Epoch 5/10\n",
      "current SGD learning rate = 0.00407253125\n",
      "average train loss = 6.36124\n",
      "average val loss = 7.11477\n",
      "__________________________________________________\n",
      "Epoch 6/10\n",
      "current SGD learning rate = 0.0038689046874999995\n",
      "average train loss = 5.97007\n",
      "average val loss = 6.00469\n",
      "__________________________________________________\n",
      "Epoch 7/10\n",
      "current SGD learning rate = 0.003675459453124999\n",
      "average train loss = 5.63459\n",
      "average val loss = 6.54717\n",
      "__________________________________________________\n",
      "Epoch 8/10\n",
      "current SGD learning rate = 0.003491686480468749\n",
      "average train loss = 5.34915\n",
      "average val loss = 5.79150\n",
      "__________________________________________________\n",
      "Epoch 9/10\n",
      "current SGD learning rate = 0.0033171021564453113\n",
      "average train loss = 5.07305\n",
      "average val loss = 5.78919\n",
      "__________________________________________________\n",
      "Epoch 10/10\n",
      "current SGD learning rate = 0.0031512470486230455\n",
      "average train loss = 4.84784\n",
      "average val loss = 5.49844\n",
      "__________________________________________________\n",
      "min best val loss so far = 5.498439495312742\n",
      "____________________________________________________________________________________________________\n",
      "Testing: lr=0.005, lr_multiplier=0.95, hidden_dim=32, n_layers=4\n",
      "__________________________________________________\n",
      "Initial LR = 0.005\n",
      "LR multipliter per epoch = 0.95000\n",
      "Number of layers = 4\n",
      "Dimensionality of hidden layers = 32\n",
      "__________________________________________________\n",
      "Epoch 1/10\n",
      "current SGD learning rate = 0.005\n",
      "average train loss = 1.80700\n",
      "average val loss = 1.72887\n",
      "__________________________________________________\n",
      "Epoch 2/10\n",
      "current SGD learning rate = 0.00475\n",
      "average train loss = 1.71045\n",
      "average val loss = 1.68136\n",
      "__________________________________________________\n",
      "Epoch 3/10\n",
      "current SGD learning rate = 0.0045125\n",
      "average train loss = 1.66961\n",
      "average val loss = 1.68404\n",
      "__________________________________________________\n",
      "Epoch 4/10\n",
      "current SGD learning rate = 0.004286875\n",
      "average train loss = 1.63892\n",
      "average val loss = 1.64383\n",
      "__________________________________________________\n",
      "Epoch 5/10\n",
      "current SGD learning rate = 0.00407253125\n",
      "average train loss = 1.61416\n",
      "average val loss = 1.63984\n",
      "__________________________________________________\n",
      "Epoch 6/10\n",
      "current SGD learning rate = 0.0038689046874999995\n",
      "average train loss = 1.59259\n",
      "average val loss = 1.61841\n",
      "__________________________________________________\n",
      "Epoch 7/10\n",
      "current SGD learning rate = 0.003675459453124999\n",
      "average train loss = 1.56914\n",
      "average val loss = 1.61084\n",
      "__________________________________________________\n",
      "Epoch 8/10\n",
      "current SGD learning rate = 0.003491686480468749\n",
      "average train loss = 1.55165\n",
      "average val loss = 1.61883\n",
      "__________________________________________________\n",
      "Epoch 9/10\n",
      "current SGD learning rate = 0.0033171021564453113\n",
      "average train loss = 1.53104\n",
      "average val loss = 1.63070\n",
      "__________________________________________________\n",
      "Epoch 10/10\n",
      "current SGD learning rate = 0.0031512470486230455\n",
      "average train loss = 1.51381\n",
      "average val loss = 1.62229\n",
      "__________________________________________________\n",
      "min best val loss so far = 1.6108389095970512\n",
      "____________________________________________________________________________________________________\n",
      "Testing: lr=0.005, lr_multiplier=0.95, hidden_dim=32, n_layers=7\n",
      "__________________________________________________\n",
      "Initial LR = 0.005\n",
      "LR multipliter per epoch = 0.95000\n",
      "Number of layers = 7\n",
      "Dimensionality of hidden layers = 32\n",
      "__________________________________________________\n",
      "Epoch 1/10\n",
      "current SGD learning rate = 0.005\n",
      "average train loss = 1.79315\n",
      "average val loss = 1.71236\n",
      "__________________________________________________\n",
      "Epoch 2/10\n",
      "current SGD learning rate = 0.00475\n",
      "average train loss = 1.70810\n",
      "average val loss = 1.68673\n",
      "__________________________________________________\n",
      "Epoch 3/10\n",
      "current SGD learning rate = 0.0045125\n",
      "average train loss = 1.66932\n",
      "average val loss = 1.64913\n",
      "__________________________________________________\n",
      "Epoch 4/10\n",
      "current SGD learning rate = 0.004286875\n",
      "average train loss = 1.64336\n",
      "average val loss = 1.66445\n",
      "__________________________________________________\n",
      "Epoch 5/10\n",
      "current SGD learning rate = 0.00407253125\n",
      "average train loss = 1.61910\n",
      "average val loss = 1.63925\n",
      "__________________________________________________\n",
      "Epoch 6/10\n",
      "current SGD learning rate = 0.0038689046874999995\n",
      "average train loss = 1.60012\n",
      "average val loss = 1.61634\n",
      "__________________________________________________\n",
      "Epoch 7/10\n",
      "current SGD learning rate = 0.003675459453124999\n",
      "average train loss = 1.58582\n",
      "average val loss = 1.61135\n",
      "__________________________________________________\n",
      "Epoch 8/10\n",
      "current SGD learning rate = 0.003491686480468749\n",
      "average train loss = 1.56709\n",
      "average val loss = 1.60169\n",
      "__________________________________________________\n",
      "Epoch 9/10\n",
      "current SGD learning rate = 0.0033171021564453113\n",
      "average train loss = 1.55286\n",
      "average val loss = 1.59886\n",
      "__________________________________________________\n",
      "Epoch 10/10\n",
      "current SGD learning rate = 0.0031512470486230455\n",
      "average train loss = 1.53627\n",
      "average val loss = 1.59478\n",
      "__________________________________________________\n",
      "min best val loss so far = 1.5947840409651175\n",
      "____________________________________________________________________________________________________\n",
      "Testing: lr=0.005, lr_multiplier=0.95, hidden_dim=64, n_layers=1\n",
      "__________________________________________________\n",
      "Initial LR = 0.005\n",
      "LR multipliter per epoch = 0.95000\n",
      "Number of layers = 1\n",
      "Dimensionality of hidden layers = 64\n",
      "__________________________________________________\n",
      "Epoch 1/10\n",
      "current SGD learning rate = 0.005\n",
      "average train loss = 9.32141\n",
      "average val loss = 8.73170\n",
      "__________________________________________________\n",
      "Epoch 2/10\n",
      "current SGD learning rate = 0.00475\n",
      "average train loss = 8.00477\n",
      "average val loss = 7.53127\n",
      "__________________________________________________\n",
      "Epoch 3/10\n",
      "current SGD learning rate = 0.0045125\n",
      "average train loss = 7.37570\n",
      "average val loss = 7.88593\n",
      "__________________________________________________\n",
      "Epoch 4/10\n",
      "current SGD learning rate = 0.004286875\n",
      "average train loss = 6.84712\n",
      "average val loss = 7.05249\n",
      "__________________________________________________\n",
      "Epoch 5/10\n",
      "current SGD learning rate = 0.00407253125\n",
      "average train loss = 6.43243\n",
      "average val loss = 6.68781\n",
      "__________________________________________________\n",
      "Epoch 6/10\n",
      "current SGD learning rate = 0.0038689046874999995\n",
      "average train loss = 6.07497\n",
      "average val loss = 7.18735\n",
      "__________________________________________________\n",
      "Epoch 7/10\n",
      "current SGD learning rate = 0.003675459453124999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      6\u001b[39m hyperparameters_to_tune = {\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m#'lr': [0.001, 0.005, 0.01],\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.005\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m     }\n\u001b[32m     18\u001b[39m N_EPOCHS = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train_loss_history, val_loss_history, best_model, best_params, best_loss = \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyperparameters_to_tune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_EPOCHS\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m avg_test_loss = evaluate_model_on(best_model, test_set)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest hyperparams are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mgrid_search\u001b[39m\u001b[34m(hyperparameters, train_set, validation_set, n_epochs)\u001b[39m\n\u001b[32m     29\u001b[39m model = MLP(n_layers=n_layers, hidden_dim=hidden_dim)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# train the model and track validation loss history\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m trained_model, train_loss_history, val_loss_history = \u001b[43mtrain_model_with_SGD\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43msgd_lr_multiplier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_multiplier\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# find the best validation loss\u001b[39;00m\n\u001b[32m     42\u001b[39m min_val_loss = \u001b[38;5;28mmin\u001b[39m(val_loss_history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtrain_model_with_SGD\u001b[39m\u001b[34m(model, training_set, validation_set, lr, n_epochs, sgd_lr_multiplier)\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m#do SGD step\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model.n_layers):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         model.layers[i] -= lr*layer_grads[i]\n\u001b[32m     55\u001b[39m     total_train_loss += loss\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#decrease lr each epoch\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#mlp = MLP()\n",
    "\n",
    "#SGD_LEARNING_RATE = 2e-3\n",
    "#LEARNING_RATE_MULTIPLIER_PER_EPOCH = 0.95\n",
    "# Define the hyperparameter grid\n",
    "hyperparameters_to_tune = {\n",
    "    #'lr': [0.001, 0.005, 0.01],\n",
    "    'lr': [0.005],\n",
    "    'lr_multiplier': [0.95],\n",
    "    #'hidden_dim': [16, 32, 64],\n",
    "    'hidden_dim': [32, 64],\n",
    "    #'hidden_dim': [8, 16],\n",
    "    'n_layers': [1, 4, 7]\n",
    "    #'n_layers': [1]\n",
    "   \n",
    "    }\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "train_loss_history, val_loss_history, best_model, best_params, best_loss = grid_search(\n",
    "        hyperparameters_to_tune,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        n_epochs=N_EPOCHS\n",
    "    )\n",
    "\n",
    "\n",
    "avg_test_loss = evaluate_model_on(best_model, test_set)\n",
    "\n",
    "print (f\"best hyperparams are {best_params}\")\n",
    "print (f\"VAL LOSS of best model is = {best_loss:.5f}\")\n",
    "print (f\"TEST LOSS of best model is = {avg_test_loss:.5f}\")\n",
    "\n",
    "log_message (\"_\" * 100)\n",
    "log_message (f\"best hyperparams are {best_params}\")\n",
    "log_message (f\"VAL LOSS of best model is = {best_loss:.5f}\")\n",
    "log_message (f\"TEST LOSS of best model is = {avg_test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d3e16",
   "metadata": {},
   "source": [
    "Let´s plot this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ab6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, train_loss_history, val_loss_history = train_model_with_SGD(\n",
    "#    model, training_set, validation_set, lr=0.01, n_epochs=20\n",
    "#)\n",
    "\n",
    "print (f\"best hyperparams are {best_params}\")\n",
    "print (f\"VAL LOSS of best model is = {best_loss:.5f}\")\n",
    "print (f\"TEST LOSS of best model is = {avg_test_loss:.5f}\")\n",
    "\n",
    "plot_loss(train_loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d5bc0394dfea0",
   "metadata": {},
   "source": [
    "Confusion matrix of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad10f6-22a8-47b8-b269-d55ae2d1922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#confusion matrix heatmap\n",
    "\n",
    "def print_confusion_matrix_of (model, test_set):\n",
    "\n",
    "    # Get predictions on test set\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "    \n",
    "    for x, y in test_set:\n",
    "        logits = model.forward(x)  # Get model predictions\n",
    "        prediction = np.argmax(logits)  # Convert to class prediction\n",
    "        test_predictions.append(prediction)\n",
    "        test_true_labels.append(y)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "    plt.title('Confusion Matrix - FER-2013 Test Set')\n",
    "    plt.ylabel('True Emotion')\n",
    "    plt.xlabel('Predicted Emotion')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_confusion_matrix_of (best_model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d343486-2b0a-444c-82e9-6b9c5f28eac1",
   "metadata": {},
   "source": [
    "A crutch aka dirty trick: downsample images in a dataset to decrease training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f9bfc-3d95-4853-91b4-c792331e6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d_grayscale(image, pool_size=2, stride=2):\n",
    "    \n",
    "    # If image has a single channel dimension, remove it\n",
    "    if image.ndim == 3 and image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "    \n",
    "    H, W = image.shape\n",
    "  \n",
    "    patches = sliding_window_view(image, (pool_size, pool_size))\n",
    "    patches = patches[::stride, ::stride, :, :]\n",
    "    pooled = patches.max(axis=(-2, -1))\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e41756-26dc-482a-b14a-ed98c9cfe27b",
   "metadata": {},
   "source": [
    "Since 48x48 images have been downsampled to 24x24, that is input dimension accepted by the CNN class by default. Since the image is that small, 7x7 and 5x5 convolutions hardly make sense, 3x3 convolutional kernel size is default in the class. The model has n_conv_layers ReLU\n",
    "\n",
    "CNN implements a varying number of convolutional layers with ReLU activation functions with the number of channels which is growing from layer to layer by a factor passed into CNN constructor. One fully connected softmax layer is hardcoded into the network to project the output of convolutional layers onto a 7-dimensional vector space whose vectors give a predicted probability distribution over the target classes. One also can pass the desired size of the convolutional kernel. Like in standard deep learning libraries, this CNN implementation uses discrete cross-correlation rather than discrete convolution in each “convolutional” layer. In this report, I use term ‘cross-correlation’ and ‘cross-correlational’ interchangeably with ‘convolution’ and ‘convolutional’.  \n",
    "\n",
    "Kaiming weight initialization has been used for ReLU layers, Xavier one for the hardcoded softmax fully connected layer. \n",
    "\n",
    "The skeleton of the CNN class is similar to the one of the MLP class. Were there more time in my disposal, I would have the MLP and the CNN class inherit some of the attributes and methods from a more abstract class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773cad2-861e-4e59-bb74-d0c920f40d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN ():\n",
    "    #def __init__(self, in_dim: int = 48, out_dim: int=7, kernel_size: int = 5, n_channels: int = 2, n_conv_layers: int = 5):\n",
    "    def __init__(self, in_dim: int = 24, out_dim: int=7, kernel_size: int = 3, n_channels: int = 1, n_chan_mult: int = 1.1, n_conv_layers: int = 3):\n",
    "        \n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "        self.input_h = in_dim\n",
    "        self.input_w = in_dim\n",
    "        self.output_dim = out_dim\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        #contains number of channels in i-th layer\n",
    "        #where 0th layer is the input layer with\n",
    "        #one channel since inputs are grayscale\n",
    "        self.n_channels = [1]\n",
    "\n",
    "        #no more conv layers are initialized \n",
    "        #than necessary so that the receptive field\n",
    "        #of the last conv layer covers the entire input image\n",
    "        self.n_conv = min(n_conv_layers, (in_dim // (kernel_size-1)) -1 )\n",
    "        \n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(self.n_conv):\n",
    "            self.n_channels.append(n_channels)\n",
    "\n",
    "            #compute standard deviation for Kaiming initialization of\n",
    "            #weights of convolutional kernels\n",
    "            #because ReLU activations are used\n",
    "            current_input_dim = n_channels * (kernel_size ** 2)\n",
    "            std = np.sqrt(2/current_input_dim )\n",
    "\n",
    "            #initialize as many kernels as there are channels\n",
    "            #those n_channels kernales are layer parameters\n",
    "            self.layers.append(rng.normal(loc=0.0, scale=std, size=(self.n_channels[i+1], self.n_channels[i], self.kernel_size, self.kernel_size)))\n",
    "\n",
    "            #increase n_channels across layers gradually \n",
    "            n_channels = math.ceil(n_chan_mult*n_channels)\n",
    "            #n_channels = math.trunc(1.1*n_channels)\n",
    "            #if n_channels < 256:\n",
    "            #    n_channels *= 2\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #sampling boundaries for uniform Xavier weight init for a fully connected layer\n",
    "        #last convolutional layer has self.n_channels[-1] channels\n",
    "        #with feature widheight and width = in_dim - n_conv * (kernel_size - 1)\n",
    "        #next output layer has out dim parameters\n",
    "        n_param_per_channel_last_conv_layer = (self.input_w - self.n_conv * (self.kernel_size - 1)) ** 2\n",
    "        n_param_total_last_conv_layer = self.n_channels[-1] * n_param_per_channel_last_conv_layer\n",
    "        uniform_boundary = np.sqrt(6/(n_param_total_last_conv_layer + self.output_dim))\n",
    "\n",
    "        #add one fully connected layer on top of convolutional ones\n",
    "        #to project output of successive convolutions onto\n",
    "        #the vector space of dimensionality out_dim\n",
    "        self.n_fc = 1\n",
    "        self.fc = rng.uniform(-uniform_boundary, uniform_boundary, size=(n_param_total_last_conv_layer, self.output_dim))\n",
    "        \n",
    "    \n",
    "    #def cross_correlation_2D_of (self, input, kernels):\n",
    "    def _cross_correlation_2D_of_with (self, kernels, input, forward_pass = True):\n",
    "       \n",
    "        picture_patches = sliding_window_view(input, kernels.shape[-2:], axis = (1,2))\n",
    "\n",
    "        #this is cc2d for the backward pass pass\n",
    "        if forward_pass:\n",
    "            cc2d = np.einsum(\"ihwkl, oikl->ohw\", picture_patches, kernels)\n",
    "            \n",
    "        #this is cc2d for the forward pass\n",
    "        else:\n",
    "            cc2d = np.einsum(\"ihwkl, okl->oihw\", picture_patches, kernels)\n",
    "\n",
    "        return cc2d\n",
    "    \n",
    "    def _transposed_cross_correlation_2D_of_with (self, error_signal, network_layer):\n",
    "\n",
    "     \n",
    "        _, _, kH, kW = network_layer.shape\n",
    "\n",
    "        pad_h, pad_w = kH - 1, kW - 1\n",
    "        # pad spatial axes only\n",
    "        error_signal_padded = np.pad(error_signal, ((0,0), (pad_h,pad_h), (pad_w,pad_w)), mode='constant')\n",
    "\n",
    "        \n",
    "        picture_patches = sliding_window_view(error_signal_padded, (kH, kW), axis = (1,2))\n",
    "\n",
    "        tcc2d = np.einsum(\"ohwkl, oikl->ihw\", picture_patches, network_layer)\n",
    "\n",
    "\n",
    "        return tcc2d\n",
    "   \n",
    "    def _ReLU (self, x):\n",
    "        return np.maximum(x, 0, out=x)\n",
    "    \"\"\"\n",
    "    def _get_normalized_logits_with_softmax_denom(self, logits):\n",
    "        softmax_denominator = np.sum(np.exp(logits))\n",
    "        return np.exp(logits) / softmax_denominator, softmax_denominator\n",
    "    \"\"\"\n",
    "    def _get_normalized_logits_with_softmax_denom(self, logits):\n",
    "        logits = logits - np.max(logits)\n",
    "        exp_logits = np.exp(logits)\n",
    "        softmax_denominator = np.sum(exp_logits)\n",
    "        return exp_logits / softmax_denominator, softmax_denominator\n",
    "   \n",
    "    def forward (self, inputs, target_value=None, requires_grad = False):\n",
    "        if len(inputs.shape) == 2:\n",
    "            inputs = np.expand_dims(inputs, axis=0)\n",
    "  \n",
    "        layer_activations = [inputs]\n",
    "        for i in range(self.n_conv):\n",
    "            cross_correlation = self._cross_correlation_2D_of_with(self.layers[i], layer_activations[i])\n",
    "            layer_activations.append(self._ReLU(cross_correlation))\n",
    "         \n",
    "\n",
    "        #push output into the fully conntected layer\n",
    "        #to get as many logits as there are\n",
    "        #target classes\n",
    "        #print (\"fc shape \", self.fc.shape)\n",
    "        #print (\"last conv flattened shape \", layer_activations[-1].flatten().shape)\n",
    "        logits =  layer_activations[-1].flatten() @ self.fc\n",
    "\n",
    "        \n",
    "        #if no target value is passed to the model.forward\n",
    "        #then the model is in the inference mode\n",
    "        #no logit/output normalization/softmax is necessary\n",
    "        #in the inference mode\n",
    "        #since one can base model prediction on the highest\n",
    "        #unnormalized/unsoftmaxed logit\n",
    "        if target_value is None:\n",
    "            return logits\n",
    "           \n",
    "        else:\n",
    "            #this is softmax\n",
    "            #softmax denominator is returned by softmax \n",
    "            #on top of normalized logits to be\n",
    "            #reused in computing CEL\n",
    "            normalized_logits, softmax_denom = self._get_normalized_logits_with_softmax_denom(logits)\n",
    "\n",
    "            \n",
    "            #CEL is equal to -ln(exp(logits[target_value])/softmax_denom))\n",
    "            #the formula below is algebraically equivalent to the one above\n",
    "            #CEL_value = -logits[target_value] + np.log(softmax_denom)\n",
    "            # normalized logits for stable computation\n",
    "            CEL_value = -np.log(normalized_logits[target_value])\n",
    "            \n",
    "        #if no gradient is required,\n",
    "        #then just return CEL\n",
    "        if not requires_grad:\n",
    "            return CEL_value\n",
    "        \n",
    "\n",
    "        #if a target value is passed to model.forward\n",
    "        #and CEL is required\n",
    "        #then the model is in the training mode\n",
    "        #one needs to do softmax on the inputs\n",
    "        #to pass softmaxed logits into \n",
    "        #CEL/cross-entropy loss\n",
    "        else:\n",
    "\n",
    "            #this one is the gradient of CEL\n",
    "            #d_softmax = normalized_logits.copy()\n",
    "            d_softmax = normalized_logits\n",
    "            d_softmax[target_value] -= 1\n",
    "\n",
    "            #gradient of the fully connected layer\n",
    "            fc_gradient = np.outer(layer_activations[-1].flatten(), d_softmax)\n",
    "\n",
    "            #initialize an list with as many empty\n",
    "            #elements as there are conv layers\n",
    "            #i.e. cross-correlation param tensors\n",
    "            conv_layer_gradients = [None] * (self.n_conv)\n",
    "            #error_signal = self.fc @ d_softmax\n",
    "\n",
    "            #!!!!!\n",
    "            reshaped_fc_shape = list(layer_activations[-1].shape)\n",
    "            reshaped_fc_shape.append(self.output_dim)\n",
    "           \n",
    "            error_signal = self.fc.reshape(reshaped_fc_shape) @ d_softmax\n",
    "       \n",
    "\n",
    "            \n",
    "            for i in range(self.n_conv-1, -1, -1):\n",
    "                \n",
    "                error_signal *= (layer_activations[i+1] > 0)\n",
    "\n",
    "                current_conv_layer_grad = self._cross_correlation_2D_of_with(\n",
    "                    error_signal, layer_activations[i], forward_pass=False\n",
    "                )\n",
    "\n",
    "         \n",
    "                error_signal = self._transposed_cross_correlation_2D_of_with(\n",
    "                    error_signal, self.layers[i]\n",
    "                )\n",
    "\n",
    "                conv_layer_gradients[i] = current_conv_layer_grad\n",
    "                \n",
    "            \n",
    "            layer_gradients = conv_layer_gradients\n",
    "            layer_gradients.append(fc_gradient)\n",
    "\n",
    "            return CEL_value, layer_gradients\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea48a7-371d-4a67-b949-0611730f8bf9",
   "metadata": {},
   "source": [
    "CNN optimizer is the same as for MLP, I lacked time to refactor the method to be able to optimize instances of both the MLP and the CNN classes.\n",
    "\n",
    "The model is trained via stochastics gradient descent. The batch size is thus equal to 1. After each epoch, the learning rate is multiplied by an antecedently specified hyperparameter whose value is constant during training and is smaller than 1. This measure is introduced so that the optimizer is not vulnerable to a scenario in which it constantly overshoots a local minimum of CEL and thus diverges therefrom. A decreasing learning rate/step size ensures that a local minimum of CEL is being overshot less and less as model training is progressing.\n",
    "\n",
    "The optimizer retuns the state of the model which achieved the lowest CEL score on the validation set as opposed to the state of the model which achieved the lowest CEL score on the validation set after n_epochs epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727a7e8-658f-49cc-a631-ccb1ba18443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN_with_SGD (model, \n",
    "                         training_set,\n",
    "                         validation_set,\n",
    "                         lr: float, \n",
    "                         n_epochs: int, \n",
    "                         sgd_lr_multiplier: float = 0.95\n",
    "                        ):\n",
    "    \n",
    "\n",
    "    print (\"_\" * 50)\n",
    "    print (f\"Initial LR = {lr}\")\n",
    "    print (f\"LR multipliter per epoch = {sgd_lr_multiplier:.5f}\")\n",
    "    print (f\"Number of conv layers = {model.n_conv}\")\n",
    "    print (f\"Number of channels in conv layers = {model.n_channels[1:]}\")\n",
    "    print (\"_\" * 50)\n",
    "\n",
    "    log_message (\"_\" * 50)\n",
    "    log_message (f\"Initial LR = {lr}\")\n",
    "    log_message (f\"LR multipliter per epoch = {sgd_lr_multiplier:.5f}\")\n",
    "    log_message (f\"Number of conv layers = {model.n_conv}\")\n",
    "    log_message (f\"Number of channels in conv layers = {model.n_channels[1:]}\")\n",
    "    log_message (\"_\" * 50)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_avg_epoch_loss = 1000\n",
    "\n",
    "    for epoch_index in range(1, n_epochs + 1):\n",
    "\n",
    "        print (f\"Epoch {epoch_index}/{n_epochs}\")\n",
    "        print (f\"current SGD learning rate = {lr}\")\n",
    "\n",
    "        log_message (f\"Epoch {epoch_index}/{n_epochs}\")\n",
    "        log_message (f\"current SGD learning rate = {lr}\")\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        #shuffle training set in a reproducible manner\n",
    "        random.seed(42 + epoch_index)\n",
    "        random.shuffle(training_set) \n",
    "        start1 = time.time()\n",
    "        for x,y in training_set:\n",
    "            #start2 = time.time()\n",
    "            #get the CEL gradient from the forward pass directly\n",
    "            loss, layer_grads = model.forward(x, y, requires_grad=True)\n",
    "         \n",
    "            #do SGD step\n",
    "            model.fc -= lr*layer_grads[-1]\n",
    "            \n",
    "            model.layers = [layer - lr * grad for layer, grad in zip(model.layers, layer_grads[:-1])]\n",
    "                 \n",
    "            #for i in range(model.n_conv):\n",
    "            #   model.layers[i] -= lr*layer_grads[i]\n",
    "           \n",
    "\n",
    "       \n",
    "            #end = time.time()\n",
    "            #rint(f\"Elapsed time per SGD iter: {end - start2} seconds\")\n",
    "            total_train_loss += loss\n",
    "        end = time.time()\n",
    "        print(f\"Elapsed time per epoch iter: {end - start1} seconds\")\n",
    "        \n",
    "        #decrease lr each epoch\n",
    "        lr *= sgd_lr_multiplier\n",
    "        \n",
    "        #compute, print and save avg loss per epoch\n",
    "        avg_train_loss = total_train_loss / len(training_set)\n",
    "        print (f\"average train loss = {avg_train_loss:.5f}\")\n",
    "        log_message (f\"average train loss = {avg_train_loss:.5f}\")\n",
    "        train_loss_history.append(avg_train_loss)\n",
    " \n",
    "        avg_val_loss = evaluate_model_on (model, validation_set)\n",
    "\n",
    "        if avg_val_loss < best_avg_epoch_loss:\n",
    "            best_avg_epoch_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "\n",
    "        print (f\"average val loss = {avg_val_loss:.5f}\")\n",
    "        log_message (f\"average val loss = {avg_val_loss:.5f}\")\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        print (\"_\" * 50)\n",
    "        log_message (\"_\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if best_model is not None:\n",
    "        return best_model, train_loss_history, val_loss_history\n",
    "    else:\n",
    "        return model, train_loss_history, val_loss_history\n",
    "    #return model, train_loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6b6f4-5e8f-405e-93dc-c98dc5f95d0c",
   "metadata": {},
   "source": [
    "What goes for SGD optimizer, also goes for grid search. In principle, those method do the same, I have no time left to make one method work for both MLP and CNN.\n",
    "\n",
    "Grid search tries out several combinations of hyperparameters in order to achieve the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c74156-de3b-411d-88db-3bd76cc7f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(hyperparameters: dict,\n",
    "                train_set: list,\n",
    "                validation_set: list,\n",
    "                n_epochs: int):\n",
    "    best_loss = float(\"inf\")\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "\n",
    "    # extract hyperparameter combinations\n",
    "    #learning_rate = hyperparameters.get('lr', [])\n",
    "    #lr_multipliers = hyperparameters.get('lr_multiplier', [])\n",
    "    #hidden_dims = hyperparameters.get('hidden_dim', [])\n",
    "    #n_layers_list = hyperparameters.get('n_layers', [])\n",
    "\n",
    "    n_conv_layers = hyperparameters.get('n_conv_layers', [])\n",
    "    n_chan_mult = hyperparameters.get('n_chan_mult', [])\n",
    "\n",
    "\n",
    "    #hyperparameters_to_tune = {\n",
    "    #'n_conv_layers': [3, 5, 7],\n",
    "    #'n_chan_mult': [1.1, 1.3],\n",
    "\n",
    "    lr = SGD_LEARNING_RATE\n",
    "    lr_multiplier = LEARNING_RATE_MULTIPLIER_PER_EPOCH\n",
    "    n_channels = 4\n",
    "\n",
    "\n",
    "    for n_conv in n_conv_layers:\n",
    "        for chan_mult in n_chan_mult:\n",
    "            print (\"_\" * 100)\n",
    "            #print(f\"Testing: lr={lr}, lr_multiplier={lr_multiplier}, \"\n",
    "            #        f\"number of conv layers={n_conv}, number of channels in first conv layer = {n_channels}\",\n",
    "            #        f\"multiplier of number of channels with each subsequent layer = {chan_mult}\")\n",
    "            print (f\"Testing: lr={lr}, lr_multiplier={lr_multiplier}\")\n",
    "            print (f\"number of conv layers={n_conv}, number of channels in first conv layer = {n_channels}\")\n",
    "            print (f\"multiplier of number of channels with each subsequent layer = {chan_mult}\")\n",
    "          \n",
    "            \n",
    "            log_message (\"_\" * 100)\n",
    "            log_message (f\"Testing: lr={lr}, lr_multiplier={lr_multiplier}\")\n",
    "            log_message(f\"number of conv layers={n_conv}, number of channels in first conv layer = {n_channels}\")\n",
    "            log_message(f\"multiplier of number of channels with each subsequent layer = {chan_mult}\")\n",
    "          \n",
    "            # create a new model for each combination\n",
    "\n",
    "            model = CNN(in_dim = 24, out_dim=7, kernel_size = 3, n_channels = n_channels, n_chan_mult = chan_mult, n_conv_layers = n_conv)\n",
    "        \n",
    "            \n",
    "            #model = MLP(n_layers=n_layers, hidden_dim=hidden_dim)\n",
    "\n",
    "            # train the model and track validation loss history\n",
    "            trained_model, train_loss_history, val_loss_history = train_CNN_with_SGD(\n",
    "                model,\n",
    "                train_set,\n",
    "                validation_set,\n",
    "                lr=lr,\n",
    "                n_epochs=n_epochs,\n",
    "                sgd_lr_multiplier=lr_multiplier\n",
    "            )\n",
    "\n",
    "            # find the best validation loss\n",
    "            min_val_loss = min(val_loss_history)\n",
    "            #print (f\"min_val_loss = {min_val_loss:.5f}\")\n",
    "\n",
    "            # check if new combination is best\n",
    "            if min_val_loss < best_loss:\n",
    "                best_loss = min_val_loss\n",
    "                best_params = {\n",
    "                    'lr': lr,\n",
    "                    'lr_multiplier': lr_multiplier,\n",
    "                    'n_channels': n_channels,\n",
    "                    'n_chan_mult': chan_mult,\n",
    "                    'n_conv_layers': n_conv\n",
    "                }\n",
    "                best_model = trained_model\n",
    "                best_train_loss_history = train_loss_history\n",
    "                best_val_loss_history = val_loss_history\n",
    "            \n",
    "            print (f\"min best val loss so far = {best_loss}\")\n",
    "            log_message (f\"min best val loss so far = {best_loss}\")\n",
    "        \n",
    "    return best_train_loss_history, best_val_loss_history, best_model, best_params, best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa222cf0-c2a4-4fa7-9929-abe7023717eb",
   "metadata": {},
   "source": [
    "Due to large training time of CNNs via SGD, I found a good learning rate as well as learning rate decay rate manually (0.0015 and 0.95 respectively).\n",
    "\n",
    "I optimized for the number of convolutional layers as well as for the factor with which the number of channels grows from conv layer to conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6cc30-4330-492d-827b-2dc2521968cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"cnn_grid_search_log_{timestamp}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"=== New Log Start ===\\n\")\n",
    "       \n",
    "\n",
    "train_set, val_set, test_set = preprocess_dataset_for(\"cnn\")\n",
    "\n",
    "train_set = [(maxpool2d_grayscale(img), label) for img, label in train_set]\n",
    "val_set   = [(maxpool2d_grayscale(img), label) for img, label in val_set]\n",
    "test_set  = [(maxpool2d_grayscale(img), label) for img, label in test_set]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SGD_LEARNING_RATE = 15e-4\n",
    "LEARNING_RATE_MULTIPLIER_PER_EPOCH = 0.95\n",
    "N_EPOCHS = 5\n",
    "\n",
    "hyperparameters_to_tune = {\n",
    "    'n_conv_layers': [1, 3, 6],\n",
    "    #'n_conv_layers': [2],\n",
    "    'n_chan_mult': [1.1, 1.2]\n",
    "    }\n",
    "\n",
    "\n",
    "train_loss_history, val_loss_history, best_model, best_params, best_loss = grid_search(\n",
    "        hyperparameters_to_tune,\n",
    "        list(train_set),\n",
    "        list(val_set),\n",
    "        n_epochs=N_EPOCHS\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cnn_trained, train_loss_history, val_loss_history = train_CNN_with_SGD (cnn,\n",
    "#                                            list(train_set),\n",
    "#                                            list(val_set),\n",
    "#                                            SGD_LEARNING_RATE,\n",
    "#                                            N_EPOCHS,\n",
    "#                                            LEARNING_RATE_MULTIPLIER_PER_EPOCH\n",
    "#                                            )\n",
    "#avg_test_loss = evaluate_model_on(cnn_trained, list(test_set))\n",
    "#print (f\"TEST LOSS = {avg_test_loss:.5f}\")\n",
    "#log_message (f\"TEST LOSS = {avg_test_loss:.5f}\")\n",
    "#print (\"_\" * 50)\n",
    "#log_message (\"_\" * 50)\n",
    "\n",
    "avg_test_loss = evaluate_model_on(best_model, list(test_set))\n",
    "print (f\"TEST LOSS = {avg_test_loss:.5f}\")\n",
    "log_message (f\"TEST LOSS = {avg_test_loss:.5f}\")\n",
    "print (\"_\" * 50)\n",
    "log_message (\"_\" * 50)\n",
    "\n",
    "print (f\"best hyperparams are {best_params}\")\n",
    "print (f\"VAL LOSS of best model is = {best_loss:.5f}\")\n",
    "print (f\"TEST LOSS of best model is = {avg_test_loss:.5f}\")\n",
    "\n",
    "log_message (\"_\" * 100)\n",
    "log_message (f\"best hyperparams are {best_params}\")\n",
    "log_message (f\"VAL LOSS of best model is = {best_loss:.5f}\")\n",
    "log_message (f\"TEST LOSS of best model is = {avg_test_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699ad8c-4877-4fdd-ae2d-c490444a4726",
   "metadata": {},
   "source": [
    "Train a CNN with \"optimal\" hyperparameters longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6a3be-db28-4672-9c3a-8cc28f999136",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 4\n",
    "new_cnn = CNN(in_dim = 24,\n",
    "            out_dim=7, \n",
    "            kernel_size = 3, \n",
    "            n_channels = best_params['n_channels'], \n",
    "            n_chan_mult = best_params['chan_mult'],\n",
    "            n_conv_layers = best_params['n_conv_layers'])\n",
    "        \n",
    "new_cnn_trained, train_loss_history, val_loss_history = train_CNN_with_SGD (new_cnn,\n",
    "                                            list(train_set),\n",
    "                                            list(val_set),\n",
    "                                            best_params['lr'],\n",
    "                                            10,\n",
    "                                            best_params['lr_multiplier']\n",
    "                                            )\n",
    "avg_test_loss = evaluate_model_on(new_cnn_trained, list(test_set))\n",
    "print (f\"TEST LOSS = {avg_test_loss:.5f}\")\n",
    "print (\"_\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e89658-8390-4810-b5a5-aa9775171649",
   "metadata": {},
   "source": [
    "Plot best CNN model loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b02e6-803e-41ce-b9d4-7e68e3282f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_loss(train_loss_history, val_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a3d0e-9f12-418a-b44c-898df91c111e",
   "metadata": {},
   "source": [
    "Plot CNN confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29903f-1c73-46ef-a58c-4ea8c8058483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_confusion_matrix_of (best_model, test_set)\n",
    "print_confusion_matrix_of (new_cnn_trained, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a056fe-ae85-4877-a2c5-717cffb4d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a36ff-d895-4074-be5c-7e905ce5567e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3932e-b0e5-43de-95b1-fd3879c24716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c77b4-0a93-4633-a724-97597f9d40d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd4ef8-3eea-41d5-b2c5-649bbd607924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
